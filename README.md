# aws-redshift-data-project

Um **pipeline de dados ELT (Extract, Load, Transform) completo na AWS**, que orquestra a ingest√£o de **milh√µes de registros** de dados brutos do **Amazon S3** para o **Amazon Redshift** via **Python**, onde poderosas an√°lises e transforma√ß√µes **SQL** s√£o executadas. Os resultados s√£o exibidos de forma clara com **Pandas**, e a automa√ß√£o do fluxo √© garantida por **GitHub Actions**.

---

# üöÄ Projeto de An√°lise de Dados com AWS Redshift e ELT

Este projeto demonstra um pipeline robusto de an√°lise de dados, utilizando as capacidades de um Data Warehouse na nuvem, o **Amazon Redshift**, e o poderoso conceito de **ELT (Extract, Load, Transform)**. O objetivo √© ingerir dados de diferentes fontes, armazen√°-los de forma otimizada e, em seguida, realizar an√°lises complexas para extrair insights valiosos.

---

## üåü Tecnologias Utilizadas

Este projeto alavanca um conjunto de tecnologias AWS e ferramentas Python para construir um fluxo de dados eficiente:

* **Amazon Redshift:** O cora√ß√£o do nosso Data Warehouse, um servi√ßo de banco de dados colunar totalmente gerenciado que permite consultas anal√≠ticas r√°pidas e poderosas em grandes volumes de dados. √â o ambiente principal para a fase de **Transforma√ß√£o** do ELT.
* **Amazon S3 (Simple Storage Service):** Utilizado como um Data Lake e √°rea de *staging* para armazenar os dados brutos (arquivos CSV) antes de serem carregados no Redshift. Essencial para a fase de **Extra√ß√£o** e **Carregamento** do ELT.
* **AWS IAM (Identity and Access Management):** Garante a seguran√ßa e o controle de acesso aos recursos da AWS, definindo as permiss√µes necess√°rias para que o Redshift possa acessar o S3 (via IAM Role) e para que o script possa interagir com ambos os servi√ßos.
* **Python:** A linguagem de programa√ß√£o principal para orquestrar o pipeline, manipular arquivos e interagir com as APIs da AWS.
* **SQL (Structured Query Language):** A linguagem padr√£o para gerenciar e manipular dados em bancos de dados relacionais. √â crucial para criar tabelas, carregar dados (via `COPY`) e, principalmente, para realizar todas as transforma√ß√µes e an√°lises complexas diretamente no Redshift.
* **Boto3:** O SDK oficial da AWS para Python, utilizado para interagir programaticamente com o S3 (para upload de arquivos) e o Redshift (para execu√ß√£o de queries).
* **Pandas:** Uma biblioteca poderosa para manipula√ß√£o e an√°lise de dados em Python, utilizada para formatar e exibir os resultados das queries do Redshift de forma tabular e em JSON.

---

## üìä O Conceito de ELT (Extract, Load, Transform) no Projeto

Diferente do ETL tradicional, onde os dados s√£o transformados *antes* do carregamento, o ELT inverte essa ordem, aproveitando o poder computacional do pr√≥prio Data Warehouse. Este projeto segue o padr√£o ELT atrav√©s do seguinte fluxo:

### 1. Extract (Extra√ß√£o)

Nesta fase, coletamos os dados brutos de suas fontes:

* **Dados Locais:** Arquivos CSV contendo informa√ß√µes de `vendedores`, `produtos`, `pedidos` e `itens_pedido` s√£o extra√≠dos do sistema de arquivos local do projeto.
* **Dados de Grande Escala:** O projeto tamb√©m extrai uma vasta base de dados de voos (`flights`, `aircraft`, `airports`) diretamente de um bucket S3 p√∫blico da AWS (us-west-2-aws-training). Isso simula um cen√°rio real de dados de grande volume, com quase 100 milh√µes de registros.
* **Staging no S3:** Todos esses arquivos CSV, em seu formato original e sem transforma√ß√µes significativas, s√£o enviados para um bucket S3 dedicado. O S3 funciona como uma √°rea de *landing* para os dados brutos, servindo como a fonte para o pr√≥ximo passo.

### 2. Load (Carregamento)

Aqui, os dados brutos s√£o transferidos para o Data Warehouse:

* **Conex√£o ao Redshift:** O script Python estabelece uma conex√£o segura com o cluster Amazon Redshift.
* **Cria√ß√£o de Tabelas:** Tabelas s√£o criadas no Redshift com esquemas que correspondem aos dados brutos.
* **Carregamento Direto via `COPY`:** Os dados s√£o carregados de forma eficiente e em massa do S3 diretamente para as tabelas rec√©m-criadas no Redshift usando o comando `COPY`. Este comando √© otimizado para ingest√£o de grandes volumes, delegando o processamento inicial do arquivo (como ignorar cabe√ßalhos, remover aspas e usar delimitadores) ao pr√≥prio Redshift, sem a necessidade de um ambiente de transforma√ß√£o intermedi√°rio.

### 3. Transform (Transforma√ß√£o e An√°lise)

Uma vez que os dados brutos est√£o no Redshift, as transforma√ß√µes complexas s√£o realizadas:

* **Transforma√ß√µes P√≥s-Carga:** Todas as opera√ß√µes de limpeza, agrega√ß√£o, filtragem e combina√ß√£o s√£o executadas diretamente no Data Warehouse, utilizando o poder do SQL e o processamento massivo paralelo (MPP) do Redshift. Exemplos incluem:
    * **Agrega√ß√µes:** C√°lculos como `COUNT`, `SUM`, `AVG` para entender a quantidade de produtos por condi√ß√£o, o volume de vendas por estado ou o total de vendas por m√™s.
    * **Jun√ß√µes:** Combina√ß√£o de dados de diferentes tabelas (`INNER JOIN`) para obter informa√ß√µes mais ricas, como identificar os melhores vendedores ou as aeronaves com mais voos.
    * **Filtros:** Sele√ß√£o de subconjuntos de dados relevantes (ex: vendas de 2020).
    * **Cria√ß√£o de Views:** Defini√ß√£o de `VIEWS` (como `vegas_flights`) para simplificar e pr√©-organizar dados para futuras consultas anal√≠ticas.
* **Gera√ß√£o de Insights:** As queries de an√°lise produzem resultados formatados (usando Pandas e JSON para visualiza√ß√£o clara), fornecendo insights acion√°veis sobre os dados de neg√≥cio e os dados de voo.

Este fluxo ELT otimiza o uso dos recursos da nuvem, permitindo que os dados sejam ingeridos rapidamente e transformados em escala, diretamente no ambiente onde ser√£o consultados para intelig√™ncia de neg√≥cios.

---

### Passos para Configurar o Ambiente AWS

## Cria√ß√£o da Role

1. Efetue o login na plataforma da AWS e acesse o menu IAM;
2. Acesse o menu Roles no menu lateral;
3. Clique no bot√£o "Create Role";
4. No combo de Service or use case, selecione a op√ß√£o "Redshift";
5. Em Use case mantenha a op√ß√£o "Redshift - Customizable" marcada e clique no bot√£o "Next";
6. Adicione as seguintes permiss√µes: (AmazonS3FullAccess, AmazonEC2FullAccess, AmazonRedshiftFullAccess, AmazonDMSRedshiftS3Role, AmazonRedshiftDataFullAccess e AmazonRedshiftQueryEditor) e clique no bot√£o "Next";
7. Atribua o nome e descri√ß√£o a sua escolha, por exemplo no nome "Redshift-Role" e na descri√ß√£o "Role created for Redshift" e ap√≥s isso no final da p√°gina, clique no bot√£o "Create role";
8. Acesse os detalhes da role criada e copie o valor do campo ARN;

## Cria√ß√£o da VPC

1. Acesse o menu VPC;
2. Acesse o menu "Your VPCs" no menu lateral;
3. Clique no bot√£o "Create VPC";
4. Em "Resources to create", selecione a op√ß√£o "VPC and more";
5. Em "Name tag auto-generation" digite um nome a sua escolha como por exemplo "vpc-redshift";
6. No final da p√°gina clique em "Create VPC";

## Cria√ß√£o Security Group

1. Volte ao menu "Your VPCs" e clique no item de menu "Security Groups" do menu lateral;
2. Clique no bot√£o "Create security group";
3. Em "Security group name" atribua um nome e uma descri√ß√£o a sua escolha como por exemplo "security-group-redshift";
4. Em "VPC" selecione a mesma que voc√™ criou;
5. Em "Inbound rules" adicione uma regra clicando em "Add rule";
6. Em "Port range" adicione o valor "5439" e em "CIDR Blocks" coloque o valor de "0.0.0.0/0";
7. Clique em "Create security group";

## Cria√ß√£o do Cluster do Amazon Redshift

1. Acesse o menu "Amazon Redshift"''
2. Acesse o item de menu "Provisioned clusters dashboard" atrav√©s do menu lateral;
3. Clique no bot√£o "Create cluster";
4. Em "Cluster identifier" defina um nome a sua escolha como por exemplo "redshift-cluster-meu-teste";
  4.1. Tome cuidado para atribuir um nome diferente de anteriores caso voc√™ j√° tenha criado antes algum cluster, pode acontecer conflitos no momento da conex√£o por conta de algum cache interno da AWS;
5. Em "Choose the size of the cluster" mantenha a op√ß√£o "I'll choose";
6. Em "Node type", para o nosso teste selecione a op√ß√£o "ra3.large", abaixo segue uma breve descri√ß√£o sobre a diferen√ßa entre o ra3 e o dc2:
```
- RA3 o armazenamento acontece fora do redshift, a vantagem √© que o mesmo pode crescer o armazenamento sem limites, pois √© usado o S3, em contra partida √© um pouco mais lento;

- DC2 tem mais performance, √© executado dentro do storage SSD no pr√≥prio cluster do redshift, por√©m para crescer o armazenamento √© necess√°rio crescer o n√∫mero de m√°quinas, e dependendo do escopo isso pode sair mais caro;

Observa√ß√£o: Se correr o risco dos dados aumentarem muito r√°pido √© melhor o RA3, se for mais controlado, √© melhor a DC2 que vai te dar mais performance;
```
7. Em "Database configurations", atribua o nome de um usu√°rio em "Admin user name";
8. Em "Admin password" selecione a op√ß√£o "Manually add the admin password" e digite uma senha no campo que aparecer;
9. Em "Cluster permissions", clique no bot√£o "Associate IAM roles";
10. Selecione a Role que foi criada anteriormente e clique em "Associate IAM roles";
11. Em "Additional configurations" desmarque a op√ß√£o "Use defaults";
12. Em "Network and security", clique no bot√£o "Create new subnet group";
13. Na aba que se abriu, no campo Name e Description, adicione um valor a sua escolha como por exemplo "subnet-redshift";
14. Em "VPC", selecione o VPC criado anteriormente;
15. Clique no bot√£o "Add all the subnets for this VPC";
16. Clique no bot√£o "Create cluster subnet group";
17. Retorne na aba anterior e clique no bot√£o onde tem uma flexa rotat√≥ria para atualizar;
18. Mais acima no mesmo bloco de informa√ß√µes selecione o nome da VPC criada anteriormente em "Virtual private cloud (VPC)";
19. Em "VPC security groups" adicione o mesmo criado anteriormente e demarque o default;
20. Habilite a op√ß√£o "Turn on Publicly accessible";
21. Clique em "Create cluster" no final da p√°gina;
22. Aguarde alguns minutos at√© que a AWS tenha conclu√≠do a cria√ß√£o do cluster do Redshift;

## Cria√ß√£o do bucket do S3

1. Acesse o menu S3;
2. Clique no bot√£o "Create bucket";
3. Em "Bucket name" atribua um nome a sua escolha como por exemplo "bucket-redshift";
4. Clique no bot√£o "Create bucket" no final da tela;

## Criar usu√°rio para upload de arquivos no S3

1. Acesse o menu "IAM";
2. Clique em "Users" no menu lateral;
3. Clique em "Create user";
4. Defina um nome em "User name" a sua escolha como por exemplo "s3-user" e clique no bot√£o "Next";
5. Marque a op√ß√£o "Attach policies directly" na tela que se abriu;
6. Selecione a op√ß√£o "AmazonS3FullAccess" em "Permissions policies" e clique em "Next";
7. Clique em "Create user";
8. Acesse os detalhes do seu usu√°rio e clique na aba "Security credentials";
9. No bloco de informa√ß√µes "Access keys" clique no bot√£o "Create access key";
10. Marque a op√ß√£o "Command Line Interface (CLI)" e clique em "Next";
11. Em "Description tag value" atribua um nome a sua escolha como por exemplo "access-key-for-upload-s3" e clique em "Create access key";
12. Copie e deixe salvo em algum lugar por enquanto o valor dos campos "Access key" e "Secret access key";
13. Ap√≥s isso clique em "Done";

### Executar o pipeline de dados

## Informa√ß√µes de var√≠aveis de ambiente

1. Acesse os detalhes do cluster Redshift criado e copie o valor do campo Endpoint;
2. O valor das var√≠veis de ambiente dever√° conforme o dicion√°rio abaixo:
```
REDSHIFT_HOST=<Valor do campo Endpoint, remova o valor da porta e do banco do valor, o mesmo deve ficar semelhante a isso: redshift-cluster-1.cv4cutjvp7fa.us-east-2.redshift.amazonaws.com>
REDSHIFT_PORT=5439
REDSHIFT_DBNAME=dev
REDSHIFT_USER=<Nome do Usu√°rio atribuido no campo Admin user name na cria√ß√£o do Redshift>
REDSHIFT_PASSWORD=<Senha do Usu√°rio atribuido no campo Admin password na cria√ß√£o do Redshift>
BUCKET_NAME=<Nome do bucket criado>
AWS_ACCESS_KEY_ID=<Access key copiada na cria√ß√£o do usu√°rio>
AWS_SECRET_ACCESS_KEY=<Secret access key copiada na cria√ß√£o do usu√°rio>
AWS_IAM_ROLE=<Valor copiado de ARN nos detalhes da Role criada>
```

## Local

1. Crie as var√≠aveis de ambiente com as informa√ß√µes detalhadas acima;
2. Execute os comandos abaixo:
```
python3 -m venv venv

source venv/bin/activate  # Unix/macOS
# ou
venv\Scripts\activate  # Windows

pip install -r requirements.txt

python3 ./src/main_pipeline.py
```

## Github Actions

1. Fa√ßa um fork do projeto;
2. Acesse o menu Settings do seu reposit√≥rio;
3. Acesse o menu Secrets and variables->Actions;
4. Cadastre as Repository secrets com as informa√ß√µes detalhadas acima e mais essa:
```
AWS_REGION=<regi√£o onde seu ambiente foi montado, por exemplo: us-east-2>
```
5. Qualquer commit feito na branch main far√° com que o pipeline do Github Actions seja executado;

